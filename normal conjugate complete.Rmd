---
title: "Untitled"
author: "Lorenzo Ricciardulli"
date: "2024-07-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(GGally)
library(leaps)
library(ISLR)
library(readxl)
library(combinat)
library(mvtnorm)
library(coda)
library(corrplot)
library(dplyr)
library(tidyr)
library(rjags)
library(rstan)
library(R2jags)
library(bbricks)
library(LaplacesDemon)
```

# DATA CLEANING

```{r}
data = read_excel('energy efficiency.xlsx', sheet = 1)
data = data[,-10]
y = data$heating.load 
data_X = data[,1:8] 
```

# DATA VISUALIZATION

```{r}
hist(data$heating.load)
```
## Correlation matrix

```{r}
selected_variables <- c("relative.compactness", "surface.area", "wall.area", "roof.area", "overall.height", "glazing.area" ,"heating.load")
selected_data <- data[, selected_variables]
correlation_matrix <- cor(selected_data)
corrplot(correlation_matrix[, 1:6], tl.col = 'darkblue')
```

# REGRESSION

## Marginal and posterior functions

```{r}
posterior_linear_conjugate = function(y, X, beta.0, V, a, b, S){
  library(mvtnorm)
  n = nrow(X)
  p = ncol(X)
  V.n    = V + t(X)%*%X
  beta.n = solve(V + t(X)%*%X)%*%(V%*%beta.0 + t(X)%*%y)
  a.n = a + n
  b.n = b + t(y)%*%y + beta.0%*%V%*%beta.0 - t(beta.n)%*%(V + t(X)%*%X)%*%beta.n
  beta_post   = matrix(NA, S, p)
  sigma2_post = matrix(NA, S, 1)
  for(s in 1:S){
    tau  = rgamma(1, a.n/2, b.n/2)
    sigma2 = 1/tau
    beta = c(rmvnorm(1, beta.n, solve(V.n)/tau))
    beta_post[s,]   = beta
    sigma2_post[s,] = sigma2
  }
  return(posterior = list(beta_post   = beta_post,
                          sigma2_post = sigma2_post))
}

marg_like_regr = function(y, X, beta.0, V, a, b){
  X = as.matrix(X)
  V = as.matrix(V)
  n = nrow(X)
  p = ncol(X)
  V.n    = V + t(X)%*%X
  beta.n = solve(V + t(X)%*%X)%*%(V%*%beta.0 + t(X)%*%y)
  a.n = a + n
  b.n = b + t(y)%*%y + beta.0%*%V%*%beta.0 - t(beta.n)%*%(V + t(X)%*%X)%*%beta.n
  m = -n/2*log(2*pi) + a/2*log(b/2) - a.n/2*log(b.n/2) +
        lgamma(a.n/2) - lgamma(a/2) +
          log(det(V))/2 - log(det(V.n))/2
  return(m)
}
```

## Sampling posterior of beta comma tau given y

```{r}
X = model.matrix(y ~., data_X)
p = ncol(X)
head(X)
beta.0 = rep(0, p)
V      = diag(0.01, p)
a      = 0.01
b      = 0.01
S = 5000
out = posterior_linear_conjugate(y, X, beta.0, V, a, b, S = S)
str(out)
beta_post = out$beta_post
sigma2_post = out$sigma2_post

beta_post_mcmc = as.mcmc(beta_post)
sigma2_post_mcmc = as.mcmc(sigma2_post)
```

## Plots of beta

```{r}
par(mar = c(3,5,1,1))

boxplot(out$beta_post, outline = F, ylim = c(-550,550), ylab = expression(beta), las = 1)
abline(h = 0, col = "blue", lty = "dashed")
out_lm = summary(lm(y ~ X - 1))
points(out_lm$coefficients[,1], col = "blue", pch = 16)

par(mar = c(3,5,1,1))

boxplot(out$beta_post, outline = F, ylim = c(-80,80), ylab = expression(beta), las = 1)
abline(h = 0, col = "blue", lty = "dashed")
out_lm = summary(lm(y ~ X - 1))
points(out_lm$coefficients[,1], col = "blue", pch = 16)
```
## Credible intervals

```{r}
post.means = apply(beta_post_mcmc[,1:9],2, mean)
CI = apply(beta_post_mcmc[,1:9], 2, function(x) quantile(x, c(0.025, 0.975)))

posterior_draws = pivot_longer(as.data.frame(beta_post_mcmc[,1:9]), cols = 1:9) %>% 
  arrange(name) %>% 
  mutate(density = "posterior")
head(posterior_draws)
colnames(posterior_draws) = c("parameter", "value", "density")

prior_draws = data.frame(parameter = rep(c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8", "V9"), each = 1000),
                         value = rnorm(9 * 1000, 0, 100),
                         density = rep("prior", 9 * 1000))

draws = rbind(posterior_draws, prior_draws)

# Plot the densities
p1 = ggplot(draws, aes(x = value, colour = density, fill = density)) + 
  geom_density(alpha = 0.2) + xlim(c(-50,50)) +
  theme(aspect.ratio = 1) + 
  theme_bw() + 
  scale_fill_manual(values = c("prior" = "darkgreen", "posterior" = "red")) + 
  scale_color_manual(values = c("prior" = "darkgreen", "posterior" = "red")) + 
  facet_wrap(~parameter, scales = "free") + 
  labs(title = "Prior and Posterior Densities of Coefficients",
       x = "Value", y = "Density"); p1
```

# DIAGNOSTIC

## Trace plot

```{r}
beta_post_mcmc   = as.mcmc(out$beta_post)
sigma2_post_mcmc = as.mcmc(out$sigma2_post)

# Traccia i plot
traceplot(beta_post_mcmc)
traceplot(sigma2_post_mcmc)
```

## Geweke test

```{r}
# Calcolare il test di Geweke per ciascun parametro
geweke_beta <- geweke.diag(beta_post_mcmc)
geweke_sigma2 <- geweke.diag(sigma2_post_mcmc)
# Estrai i valori z
z_scores_beta <- geweke_beta$z
z_scores_sigma2 <- geweke_sigma2$z
# Convertire i risultati in data frame per ggplot2
df_beta <- data.frame(Parameter = rep(paste0("Beta[", 1:9, "]"), each = 5000), Z_Score = as.vector(z_scores_beta))
df_sigma2 <- data.frame(Parameter = "Sigma2", Z_Score = as.vector(z_scores_sigma2))

# Combinare i data frame
df_combined <- rbind(df_beta, df_sigma2)

# Grafico dei valori z del test di Geweke
ggplot(df_combined, aes(x = Parameter, y = Z_Score)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = c(-1.96, 1.96), linetype = "dashed", color = "red") +
  labs(title = "Geweke Diagnostic Z-Scores",
       x = "Parameter",
       y = "Z-Score") +
  theme_minimal()

```

# MODEL SELECTION

```{r}
## Find all the possible distinct models with q predictors, q = 1,2,3,...



set = 2:p # labels of variables (intercept, i.e. variable 1 is always included)

combn(set, 1)

1:length(set)

comb = unlist(lapply(1:length(set), combinat::combn, x = set, simplify = FALSE), recursive = FALSE)

tail(comb)

## Obs: the intercept X[,1] must be included in each the 2^9 models

## We add it and also include the model with the intercept only

all.comb = mapply(append, 1, comb, SIMPLIFY = FALSE)

all.models.index = append(1, all.comb)
head(all.models.index)


##################################
## Compute marginal likelihoods ##
##################################

all.log.marg.like = c()

K = length(all.models.index)

beta.0 = rep(0, p)
V      = diag(0.01, p)
a      = 0.01
b      = 0.01

k = 1

for(k in 1:K){
  
  set.k = all.models.index[[k]]
  
  all.log.marg.like[k] = marg_like_regr(y, X[,set.k], beta.0[set.k], V[set.k,set.k], a, b)
  
}

## Obs: we assume the uniform prior on M_k

all.marg.like = exp(all.log.marg.like)

post.model.probs = all.marg.like/sum(all.marg.like)


# alternative: use scale!

scale(all.log.marg.like, center = TRUE, scale = FALSE)

const = mean(all.log.marg.like)

post.model.probs = exp(all.log.marg.like - const)/sum(exp(all.log.marg.like - const))

names(post.model.probs) = all.models.index

head(post.model.probs)

round(post.model.probs, 3)

head(sort(post.model.probs, decreasing = TRUE))

par(mar=c(5,12,1,1))
barplot(sort(post.model.probs, decreasing = TRUE)[1:10], horiz = TRUE, las = 2, xlim = c(0,1), col = rainbow(8))

# Compare with BSS and with R squared and adjusted R squared

#####
#####

set = 1:3

lapply(1:length(set), combinat::combn, x = set, simplify = FALSE)
lapply(1:length(set), combinat::combn, x = set, simplify = TRUE)

comb = unlist(lapply(1:length(set), combinat::combn, x = set, simplify = FALSE), recursive = FALSE)
```
## model selection based on slab and priors

```{r}
##############################################
## Create a jags object containing the data ##
##############################################

library(R2jags)

jags_data = with(data, list(y = y, X = X, n = length(y), p = ncol(X)))

#################################
## Write likelihood and priors ##
#################################

logistic_regr_jags = function(){
  
  
  
  # Gamma prior to tau
  
  tau ~ dgamma(0.01, 0.01)
  
  # Likelihood:
  
  for(i in 1:n){
    
    y[i] ~ dnorm(mu[i], tau)
    mu[i] = (gamma*beta)%*%X[i,]
  }
  
  # Priors:
  
  for(j in 1:p){
    
    beta[j] ~ dnorm(0, tau*0.01)
    
  }
  
  for(j in 1:p){
    
    gamma[j] ~ dbern(w)
    
  }
  
  w ~ dbeta(1, 1)
  
}


#########################################
## Set initial values for beta and tau ##
#########################################

init_values = function(){
  
  list(beta = rep(0, p), gamma = rep(1, p))
  
}


#########################################################################
## Specify for which parameters we want to perform posterior inference ##
#########################################################################

params = c("beta", "gamma")


####################################
## Finally, run the jags function ##
####################################


jags_posterior = jags(data = jags_data,
                      inits = init_values,
                      parameters.to.save = params,
                      model.file = logistic_regr_jags,
                      n.chains = 1,
                      n.iter = 5000,
                      n.burnin = 1000,
                      n.thin = 1)


out = jags_posterior$BUGSoutput

str(out)

## Extract samples from the posterior of beta and gamma

beta_post  = out$sims.list$beta
gamma_post = out$sims.list$gamma

head(gamma_post)

dim(gamma_post)

S = nrow(gamma_post)

## Estimate the posterior probability of inclusion of each predictor Xj
## i.e. proportion of times gammaj = 1

prob_inclusion = colMeans(gamma_post)

names(prob_inclusion) = c(colnames(data_X))

par(mfrow = c(1,1), mar = c(3,5,2,2))
barplot(prob_inclusion, col = "brown3", ylab = expression(hat(p)[j]), space = 0.4)

```
```{r}
autocorr.plot(beta_post_mcmc)
```


# PREDICTIONS

```{r}
x_new = c(1, 0.98,514.5,294,110.25,7,2,0,0)

# Compute mu



mu = beta_post_mcmc%*%x_new # S values of mu

head(mu)

y_pred = rnorm(S, mu, sqrt(sigma2_post))

hist(y_pred)
```


